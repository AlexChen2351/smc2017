{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sns.set_style()\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_size(fig):\n",
    "    fig.set_size_inches(6, 2)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. System state and measurement simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trajectories_sim(T=100):\n",
    "    x = np.zeros(T+1)\n",
    "    y = np.zeros(T+1)\n",
    "    x[0] = stats.norm.rvs(0, 1, 1)\n",
    "    y[0] = float('nan')\n",
    "    for tt in range(1,T+1):\n",
    "        x[tt] = 0.7*x[tt-1]+stats.norm.rvs(0, math.sqrt(0.1), 1)\n",
    "        y[tt] = 0.5*x[tt]+stats.norm.rvs(0, math.sqrt(0.1), 1) \n",
    "    y = y[1:T+1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 2000\n",
    "x_sim, y_sim = trajectories_sim(T=T)\n",
    "\n",
    "# Plot state trajectory\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_sim)\n",
    "\n",
    "# Plot measurement trajectory\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analytical solution of the filtering problem with the Kalman filter (KF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kalman_filter(y):\n",
    "    T = len(y)\n",
    "    \n",
    "    # System parameters\n",
    "    A = 0.7\n",
    "    C = 0.5\n",
    "    x0 = 0\n",
    "    P0 = 1\n",
    "    Q = 0.1\n",
    "    R = 0.1\n",
    "    \n",
    "    # Initialize state and covariance estimates\n",
    "    x_kf = np.zeros(T+1)\n",
    "    P_kf = np.zeros(T+1)\n",
    "    \n",
    "    x_kf[0] = x0\n",
    "    P_kf[0] = P0\n",
    "    \n",
    "    # Kalman recursions (one-dimensional without matrix multiplication)\n",
    "    for tt in range(T):\n",
    "        P_tmin = A*P_kf[tt]*A + Q\n",
    "        Kt = P_tmin*C/(C*P_tmin*C+R)\n",
    "        x_kf[tt+1] = A*x_kf[tt]+Kt*(y[tt]-C*A*x_kf[tt])\n",
    "        P_kf[tt+1] = P_tmin-Kt*C*P_tmin\n",
    "    return x_kf, P_kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_kf, P_kf = kalman_filter(y_sim)\n",
    "\n",
    "fig1=plt.figure(1)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x_sim)\n",
    "plt.plot(x_kf)\n",
    "plt.xlim(0,2000)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(x_sim)\n",
    "plt.plot(x_kf)\n",
    "plt.xlim(0,100)\n",
    "\n",
    "# set_size(fig1)\n",
    "# fig1.savefig('sim_kf.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Particle filtering solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.a Resampling strategies\n",
    "Different resampling strategies are to be considered: 1) multinomial, 2) systematic, and 3) ESS-adaptive resampling. The resampling functions take a set of normalized weights / probabilities and return the ancestor indices from $0,\\dots, N - 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinomial_resampling(ws):\n",
    "    # Determine number of elements\n",
    "    N = len(ws)\n",
    "    # Create a sample of uniform random numbers\n",
    "    u_sample = stats.uniform.rvs(size=N)\n",
    "    # Transform them appropriately\n",
    "    u = np.zeros((N,))\n",
    "    u[N-1] = np.power(u_sample[N-1],1/N)\n",
    "    for i in range(N-1,0,-1):\n",
    "        u[i-1] = u[i] * np.power(u_sample[i-1],1/i)\n",
    "        \n",
    "    # Output array\n",
    "    out = np.zeros((N,),dtype=int)\n",
    "    \n",
    "    # Find the right ranges\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while j < N:\n",
    "        total += ws[i]\n",
    "        while j<N and total >u[j]:\n",
    "            out[j] = i\n",
    "            j += 1\n",
    "            \n",
    "        # Increase weight counter\n",
    "        i += 1\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def systematic_resampling(ws):\n",
    "    # Determine number of elements\n",
    "    N = len(ws)        \n",
    "    # Output array\n",
    "    out = np.zeros((N,), dtype=int)\n",
    "    \n",
    "    # Create one single uniformly distributed number\n",
    "    u = stats.uniform.rvs()/N\n",
    "    # Find the right ranges\n",
    "    total = ws[0]\n",
    "    j = 0\n",
    "    for i in range(N):\n",
    "        while total < u:\n",
    "            j += 1\n",
    "            total += ws[j]\n",
    "            \n",
    "        # Once the right index is found, save it\n",
    "        out[i] = j\n",
    "        u = u+1/N\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def stratified_resampling(ws):\n",
    "    # Determine number of elements\n",
    "    N = len(ws)        \n",
    "    # Output array\n",
    "    out = np.zeros((N,), dtype=int)\n",
    "    \n",
    "    # Find the right ranges\n",
    "    total = ws[0]\n",
    "    j = 0\n",
    "    for i in range(N):\n",
    "        u = (stats.uniform.rvs()+i)/N\n",
    "        while total < u:\n",
    "            j += 1\n",
    "            total += ws[j]\n",
    "            \n",
    "        # Once the right index is found, save it\n",
    "        out[i] = j\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.b Bootstrap particle filter (BPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = 0.7\n",
    "C = 0.5\n",
    "Q = 0.1\n",
    "R = 0.1\n",
    "Q_sqrt = math.sqrt(Q)\n",
    "R_sqrt = math.sqrt(R)\n",
    "\n",
    "def trajectories_bpf(y, N=30, sampling='multinomial', ess_max=None):\n",
    "    # Determine the number of time steps\n",
    "    T = len(y)\n",
    "    \n",
    "    # Save the history\n",
    "    xs = np.zeros((N, T + 1))\n",
    "    ancs = np.zeros((N, T + 1), dtype=int)\n",
    "    ws = np.zeros((N, T + 1))\n",
    "    \n",
    "    # Initialisation\n",
    "    xs[:, 0] = stats.norm.rvs(0, 1, N)\n",
    "    ws[:, 0] = 1 / N * np.ones((N,))\n",
    "    ancs[:, 0] = range(N)\n",
    "    \n",
    "    # Loop through all time steps\n",
    "    for t in range(T):\n",
    "        # Resample\n",
    "        \n",
    "        # Calculate effective sample size\n",
    "        ess = 1 / np.sum(np.power(ws[:, t], 2))\n",
    "        # If ess_max is not None, only do resampling if ESS is larger\n",
    "        # than the set threshold\n",
    "        if ess_max is None or ess < ess_max:\n",
    "            if sampling == 'multinomial':\n",
    "                ancs[:, t + 1] = np.random.choice(range(N), size=N, \n",
    "                                                  replace=True, p=ws[:, t])\n",
    "            elif sampling == 'systematic':\n",
    "                ancs[:, t + 1] = systematic_resampling(ws[:, t])\n",
    "            else:\n",
    "                raise ValueError(\"Sampling scheme {} unknown.\".format(sampling))\n",
    "        else:\n",
    "            ancs[:, t + 1] = ancs[:, t]\n",
    "        \n",
    "        # Propagate\n",
    "        xs[:, t + 1] = A*xs[ancs[:, t + 1], t]+Q_sqrt*stats.norm.rvs(0, 1, N)\n",
    "        \n",
    "        # Weight\n",
    "        logws = stats.norm.logpdf(y[t], loc=C*xs[:, t + 1],\n",
    "                               scale=R_sqrt)\n",
    "        # Substract maximum\n",
    "        logws -= np.max(logws)\n",
    "        # Normalize weights to be probabilities\n",
    "        ws[:, t + 1] = np.exp(logws) / np.sum(np.exp(logws))\n",
    "        \n",
    "    return xs, ws, ancs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.c Fully adapted particle filter (FAPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = 0.7\n",
    "C = 0.5\n",
    "Q = 0.1\n",
    "R = 0.1\n",
    "Q_sqrt = math.sqrt(Q)\n",
    "R_sqrt = math.sqrt(R)\n",
    "\n",
    "def trajectories_fapf(y, N=30, sampling='multinomial', ess_max=None):\n",
    "    # Determine the number of time steps\n",
    "    T = len(y)\n",
    "    \n",
    "    # Save the history\n",
    "    xs = np.zeros((N, T + 1))\n",
    "    ancs = np.zeros((N, T + 1), dtype=int)\n",
    "    ws = 1/N*np.ones((N, T + 1))\n",
    "    \n",
    "    # Initialisation\n",
    "    xs[:, 0] = stats.norm.rvs(0, 1, N)\n",
    "    #ws[:, 0] = 1 / N * np.ones((N,))\n",
    "    ancs[:, 0] = range(N)\n",
    "    \n",
    "    # Loop through all time steps\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Compute adjustment multipliers\n",
    "        mu_nu = C*A*xs[:, t]\n",
    "        Sigma_nu = R+C*Q*C\n",
    "        nu = stats.norm.pdf(y[t], loc=mu_nu, scale=math.sqrt(Sigma_nu))\n",
    "        \n",
    "        # Compute resampling weights\n",
    "        resampling_weights = nu/np.sum(nu)\n",
    "                            \n",
    "        # Calculate effective sample size\n",
    "        ess = 1 / np.sum(np.power(resampling_weights, 2))\n",
    "        # If ess_max is not None, only do resampling if ESS is larger\n",
    "        # than the set threshold\n",
    "        if ess_max is None or ess < ess_max:\n",
    "            if sampling == 'multinomial':\n",
    "                ancs[:, t + 1] = np.random.choice(range(N), size=N, \n",
    "                                                  replace=True, p=resampling_weights)\n",
    "            elif sampling == 'systematic':\n",
    "                ancs[:, t + 1] = systematic_resampling(resampling_weights)\n",
    "            else:\n",
    "                raise ValueError(\"Sampling scheme {} unknown.\".format(sampling))\n",
    "        else:\n",
    "            ancs[:, t + 1] = ancs[:, t]\n",
    "        \n",
    "        # Propagate\n",
    "        mu_xx = A*xs[ancs[:,t+1],t]\n",
    "        K = (Q*C)/(R+C*Q*C)\n",
    "        mu_xy = mu_xx+K*(y[t]-C*mu_xx)\n",
    "        Sigma_xy = (1-K*C)*Q\n",
    "        xs[:,t+1] = mu_xy+stats.norm.rvs(0, 1, N)*math.sqrt(Sigma_xy)\n",
    "                                            \n",
    "    return xs, ws, ancs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Compare filtering solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.a Plot BPF, APF and KF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "xs_bpf, ws_bpf, ancs_bpf = trajectories_bpf(y_sim[1:T+1], N=N, sampling='multinomial')\n",
    "x_bpf = np.sum(xs_bpf*ws_bpf, axis=0)\n",
    "P_bpf = np.sum(ws_bpf*np.power(xs_bpf-x_bpf,2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "xs_fapf, ws_fapf, ancs_fapf = trajectories_fapf(y_sim[1:T+1], N=N, sampling='multinomial')\n",
    "x_fapf = np.sum(xs_fapf*ws_fapf, axis=0)\n",
    "P_fapf = np.sum(ws_fapf*np.power(xs_fapf-x_fapf,2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_kf, P_kf = kalman_filter(y_sim[1:T+1])\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(x_kf,linewidth=0.5)\n",
    "ax[0].plot(x_bpf,linewidth=0.5)\n",
    "ax[0].plot(x_kf-x_bpf,linewidth=0.5)\n",
    "\n",
    "ax[1].plot(P_kf,linewidth=0.75)\n",
    "ax[1].plot(P_bpf,linewidth=0.75)\n",
    "ax[1].plot(P_kf-P_bpf,linewidth=0.75)\n",
    "\n",
    "set_size(fig)\n",
    "fig.savefig('kf_bpf.pdf')\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,2)\n",
    "ax2[0].plot(x_kf,linewidth=0.5)\n",
    "ax2[0].plot(x_fapf,linewidth=0.5)\n",
    "ax2[0].plot(x_kf-x_fapf,linewidth=0.5)\n",
    "\n",
    "ax2[1].plot(P_kf,linewidth=0.75)\n",
    "ax2[1].plot(P_fapf,linewidth=0.75)\n",
    "ax2[1].plot(P_kf-P_fapf,linewidth=0.75)\n",
    "\n",
    "set_size(fig2)\n",
    "fig2.savefig('kf_fapf.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.b Influence of $N$, the number of particles.\n",
    "\n",
    "Influence of $N$, the number of particles on the average absolute difference between particle filter (PF) and the Kalman filter (KF) solutions, i.e. \n",
    "$$\n",
    "\\begin{align}\n",
    "V_{m} = \\frac{1}{T}\\sum_{i=1}^{T} \\left|x_{KF}-x_{PF}\\right|\\\\\n",
    "V_{\\sigma^{2}} = \\frac{1}{T}\\sum_{i=1}^{T} \\left|\\sigma^{2}_{KF}-\\sigma^{2}_{PF}\\right|,\n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$x$ and $\\sigma^{2}$ represent the estimates of the mean and variance of the filtering distribution $p(x_{t}|y_{1:T})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_dev_bpf = []\n",
    "mean_dev_fapf = []\n",
    "mean_dev_var_bpf = []\n",
    "mean_dev_var_fapf = []\n",
    "\n",
    "ns = [10,15,20,25,40,50,75,100,125,150,175,200]\n",
    "\n",
    "for N in ns:\n",
    "    print(N)\n",
    "    dev_bpf = []\n",
    "    dev_fapf = []\n",
    "    dev_var_bpf = []\n",
    "    dev_var_fapf = []\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        xs_bpf, ws_bpf, ancs_bpf = trajectories_bpf(y_sim[1:T+1], N=N, sampling='multinomial')\n",
    "        x_bpf = np.sum(xs_bpf*ws_bpf, axis=0)\n",
    "        P_bpf = np.sum(ws_bpf*np.power(xs_bpf-x_bpf,2),axis=0)\n",
    "        \n",
    "        xs_fapf, ws_fapf, ancs_fapf = trajectories_fapf(y_sim[1:T+1], N=N, sampling='multinomial')\n",
    "        x_fapf = np.sum(xs_fapf*ws_fapf, axis=0)\n",
    "        P_fapf = np.sum(ws_fapf*np.power(xs_fapf-x_fapf,2),axis=0)\n",
    "        \n",
    "        dev_bpf.append(np.mean(np.abs(x_kf-x_bpf)))\n",
    "        dev_fapf.append(np.mean(np.abs(x_kf-x_fapf)))\n",
    "        \n",
    "        dev_var_bpf.append(np.mean(np.abs(P_kf-P_bpf)))\n",
    "        dev_var_fapf.append(np.mean(np.abs(P_kf-P_fapf)))\n",
    "\n",
    "    mean_dev_bpf.append(np.mean(dev_bpf))\n",
    "    mean_dev_fapf.append(np.mean(dev_fapf))\n",
    "    mean_dev_var_bpf.append(np.mean(dev_var_bpf))\n",
    "    mean_dev_var_fapf.append(np.mean(dev_var_fapf))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(ns, mean_dev_bpf, 'o-')\n",
    "ax[0].plot(ns, mean_dev_fapf, 'o-')\n",
    "ax[1].plot(ns, mean_dev_var_bpf, 'o-')\n",
    "ax[1].plot(ns, mean_dev_var_fapf, 'o-')\n",
    "\n",
    "set_size(fig)\n",
    "fig.savefig('dev_ifo_N.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Particle genealogy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Retrieve all ancestor paths. Both the \"active\" ones that survive until the end of the time horizon and the ones that die prematurely. Plot particle genealogy. \n",
    "\n",
    "Remark: plotting the terminated paths considerably slows down plotting. To speed up the process one should comment out the for loop \"for traj in dead_particles:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(N=20, T=100, sampling='multinomial', ess_max=None):\n",
    "    xs_fapf, ws_fapf, ancs_fapf = trajectories_fapf(y_sim[1:T+1], N, \n",
    "                                    sampling, \n",
    "                                    ess_max)\n",
    "    \n",
    "    # To plot the trajectories of the dead particles in gray\n",
    "    # We have to store which ones died and save their trajectories\n",
    "    dead_particles = []\n",
    "\n",
    "    # Length of the data\n",
    "    T = xs_fapf.shape[1] - 1\n",
    "    tt = tqdm_notebook(range(1, T + 1))\n",
    "    for t in tt:\n",
    "        # It is not necessary to find the ancestor lines if no \n",
    "        # particles died\n",
    "        difference = np.setdiff1d(range(N), ancs_fapf[:, t])\n",
    "        if len(difference) == 0:\n",
    "            continue\n",
    "            \n",
    "        dead_ind = np.unique(difference)\n",
    "        traj = np.zeros((len(dead_ind), t))\n",
    "        ancestors = dead_ind\n",
    "        for s in range(t, 0, -1):\n",
    "            ancestors = ancs_fapf[ancestors, s - 1]\n",
    "            traj[:, s - 1] = xs_fapf[ancestors, s - 1]\n",
    "        dead_particles.append(traj)\n",
    "\n",
    "    # Retract the actual trajectory of all particles that survived\n",
    "    # until time T\n",
    "    traj_survived = np.zeros((N, T + 1))\n",
    "    traj_survived[:, T] = xs_fapf[:, T]\n",
    "    ancestors = range(N)\n",
    "    for t in range(T, 0, -1):\n",
    "        ancestors = ancs_fapf[ancestors, t - 1]\n",
    "        traj_survived[:, t - 1] = xs_fapf[ancestors, t - 1]\n",
    "            \n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "    for traj in dead_particles:\n",
    "        for i in range(traj.shape[0]):\n",
    "            ax.plot(traj[i, :], 'o-', linestyle='-', \n",
    "                    color='grey', markersize=0.75, lw=0.25,\n",
    "                    alpha=0.8, antialiased=True);\n",
    "\n",
    "    for i in range(N):\n",
    "        ax.plot(traj_survived[i, :], 'o-', linestyle='-', \n",
    "                markeredgecolor='r',\n",
    "                markerfacecolor='r',\n",
    "                color='k', markersize=1, lw=0.5,\n",
    "                alpha=1, antialiased=True);\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    \n",
    "    fig2, ax = plt.subplots(figsize=(8, 3))\n",
    "    \n",
    "    for i in range(N):\n",
    "        ax.plot(traj_survived[i, 1:100], 'o-', linestyle='-', \n",
    "                markeredgecolor='r',\n",
    "                markerfacecolor='r',\n",
    "                color='k', markersize=1, lw=0.5,\n",
    "                alpha=1, antialiased=True);\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    \n",
    "    \n",
    "    fig.savefig('path_fapf.png')\n",
    "    fig2.savefig('path_fapf_zoom.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.a Particle genealogy for different resampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_trajectories(N=100,T=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_trajectories(sampling='systematic', N=100,T=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_trajectories(ess_max=50, N=100,T=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
